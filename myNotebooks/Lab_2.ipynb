{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555f48c2",
   "metadata": {},
   "source": [
    "## **Laboratory Task 2**\n",
    "#### **DS Elective 4 - Deep Learning**\n",
    "\n",
    "**Name:** John Vincent Gamali <br>\n",
    "**Year & Section:** DS4A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bb0a7",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network <br>\n",
    "The feedforward neural network is one of the most basic artificial neural networks. In this ANN, the data or the input provided travels in a single direction. It enters into the ANN through the input layer and exits through the output layer while hidden layers may or may not exist. So the feedforward neural network has a front-propagated wave only and usually does not have backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cc229",
   "metadata": {},
   "source": [
    "Here is the given problem setup:\n",
    "\n",
    "<img src=\"https://i.ibb.co/kVVB48ry/Screenshot-2025-09-13-at-1-11-52-PM.png\" width=\"1200\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a861482",
   "metadata": {},
   "source": [
    "#### IMPLEMENTATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aab606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89f8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17867aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074ceb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 0.0, 1.0])\n",
    "\n",
    "W_hidden = np.array([[0.2, -0.3],\n",
    "                     [0.4,  0.1],\n",
    "                     [-0.5, 0.2]])\n",
    "\n",
    "b_hidden = np.array([-0.4, 0.2])\n",
    "\n",
    "W_out = np.array([-0.3, -0.2])\n",
    "\n",
    "b_out = 0.1\n",
    "\n",
    "y_true = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54cf83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Z_hidden = x.dot(W_hidden) + b_hidden  # weighted sums for \"hidden\"\n",
    "H = relu(Z_hidden)                      # ReLU activations for \"hidden\"\n",
    "Z_out = H.dot(W_out) + b_out            # output weighted sum\n",
    "y_hat = relu(Z_out)                     # ReLU output activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2877ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Error\n",
    "\n",
    "mse = 0.5 * (y_true - y_hat)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990586b",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891daab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_hidden = [-0.7  0.1]\n",
      "H (ReLU) = [0.  0.1]\n",
      "Z_out = 0.08\n",
      "y_hat (ReLU) = 0.08\n",
      "MSE = 0.4232\n"
     ]
    }
   ],
   "source": [
    "print(\"Z_hidden =\", Z_hidden)\n",
    "print(\"H (ReLU) =\", H)\n",
    "print(\"Z_out =\", Z_out)\n",
    "print(\"y_hat (ReLU) =\", y_hat)\n",
    "print(\"MSE =\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07095372",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "In this exercise, we computed the weighted sums of inputs and added biases for the hidden layer, then applied the ReLU activation function to introduce non-linearity. Using the hidden layer outputs, we calculated the final network output and compared the predicted result against the actual target using Mean Squared Error (MSE). \n",
    "\n",
    "This step-by-step process demonstrated how inputs are transformed into outputs in a neural network, reinforcing the flow of data—\n",
    "\n",
    "## Input → Weighted Sum + Bias → Activation → Output → Error Calculation\n",
    "\n",
    "and highlighting how each layer contributes to prediction and performance evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jbook-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}